{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70c65309",
   "metadata": {},
   "source": [
    "# Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfaa23b",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27762ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c0e61c",
   "metadata": {},
   "source": [
    "## 2. Core Feature Engineering Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1443efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_essential_features(df, horizon_hours=2):\n",
    "    \"\"\"\n",
    "    Creates 49 essential features for energy prediction\n",
    "    Args:\n",
    "        df: DataFrame with 'date' and 'Appliances' columns\n",
    "        horizon_hours: Prediction horizon (default: 2 hours)\n",
    "    Returns:\n",
    "        DataFrame with engineered features and target\n",
    "    \"\"\"\n",
    "    data = df.copy()\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    data = data.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    # Create target variable (future appliance usage)\n",
    "    data['target'] = data['Appliances'].shift(-horizon_hours)\n",
    "    \n",
    "    # Extract base time components\n",
    "    data['hour'] = data['date'].dt.hour\n",
    "    data['day_of_week'] = data['date'].dt.dayofweek\n",
    "    data['month'] = data['date'].dt.month\n",
    "    \n",
    "    # Calculate target timestamp features\n",
    "    data['target_date'] = data['date'] + pd.Timedelta(hours=horizon_hours)\n",
    "    data['target_hour'] = data['target_date'].dt.hour\n",
    "    data['target_dow'] = data['target_date'].dt.dayofweek\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8923bb",
   "metadata": {},
   "source": [
    "## 3. Time-Based Features (8 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4f312ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(data):\n",
    "    \"\"\"Adds cyclical time encodings and discrete time components\"\"\"\n",
    "    \n",
    "    # Hour cyclical encoding (captures 24-hour cycle)\n",
    "    data['hour_sin'] = np.sin(2 * np.pi * data['hour'] / 24)\n",
    "    data['hour_cos'] = np.cos(2 * np.pi * data['hour'] / 24)\n",
    "    \n",
    "    # Target hour cyclical encoding\n",
    "    data['target_hour_sin'] = np.sin(2 * np.pi * data['target_hour'] / 24)\n",
    "    \n",
    "    # Day of week cyclical encoding (captures weekly patterns)\n",
    "    data['dow_sin'] = np.sin(2 * np.pi * data['day_of_week'] / 7)\n",
    "    \n",
    "    # Month cyclical encoding (captures seasonal patterns)\n",
    "    data['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4476d223",
   "metadata": {},
   "source": [
    "## 4. Lag Features (7 features)\n",
    "######  Historical values at key intervals capture temporal dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9e75c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lag_features(data):\n",
    "    \"\"\"Creates lag features at strategic time intervals\"\"\"\n",
    "    \n",
    "    # Short-term lags (immediate past influence)\n",
    "    data['lag_1h'] = data['Appliances'].shift(1)\n",
    "    data['lag_2h'] = data['Appliances'].shift(2)\n",
    "    data['lag_3h'] = data['Appliances'].shift(3)\n",
    "    \n",
    "    # Medium-term lags (recent patterns)\n",
    "    data['lag_6h'] = data['Appliances'].shift(6)\n",
    "    data['lag_12h'] = data['Appliances'].shift(12)\n",
    "    data['lag_24h'] = data['Appliances'].shift(24)  # Yesterday same hour\n",
    "    \n",
    "    # Long-term lag (weekly pattern)\n",
    "    data['lag_168h'] = data['Appliances'].shift(168)  # Last week same hour\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b94b879",
   "metadata": {},
   "source": [
    "## 5. Rolling Mean Features (5 features)\n",
    "###### Moving averages smooth noise and capture trend levels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ce02d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_means(data):\n",
    "    \"\"\"Calculates rolling averages over multiple time windows\"\"\"\n",
    "    \n",
    "    # 3-hour rolling mean (very short-term trend)\n",
    "    data['roll_3h_mean'] = data['Appliances'].shift(1).rolling(3, min_periods=1).mean()\n",
    "    \n",
    "    # 6-hour rolling mean (short-term trend)\n",
    "    data['roll_6h_mean'] = data['Appliances'].shift(1).rolling(6, min_periods=2).mean()\n",
    "    \n",
    "    # 12-hour rolling mean (half-day trend)\n",
    "    data['roll_12h_mean'] = data['Appliances'].shift(1).rolling(12, min_periods=4).mean()\n",
    "    \n",
    "    # 24-hour rolling mean (daily baseline)\n",
    "    data['roll_24h_mean'] = data['Appliances'].shift(1).rolling(24, min_periods=8).mean()\n",
    "    \n",
    "    # 168-hour rolling mean (weekly baseline)\n",
    "    data['roll_168h_mean'] = data['Appliances'].shift(1).rolling(168, min_periods=56).mean()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a587b9",
   "metadata": {},
   "source": [
    "## 6. Rolling Extremes (6 features)\n",
    "###### Min/max values identify volatility ranges and boundaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28936f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_extremes(data):\n",
    "    \"\"\"Extracts min and max values from rolling windows\"\"\"\n",
    "    \n",
    "    # Create rolling window objects\n",
    "    rolled_6 = data['Appliances'].shift(1).rolling(6, min_periods=2)\n",
    "    rolled_12 = data['Appliances'].shift(1).rolling(12, min_periods=4)\n",
    "    rolled_24 = data['Appliances'].shift(1).rolling(24, min_periods=8)\n",
    "    rolled_168 = data['Appliances'].shift(1).rolling(168, min_periods=56)\n",
    "    \n",
    "    # 6-hour extremes (short-term range)\n",
    "    data['roll_6h_max'] = rolled_6.max()\n",
    "    data['roll_6h_min'] = rolled_6.min()\n",
    "    \n",
    "    # 12-hour extremes\n",
    "    data['roll_12h_max'] = rolled_12.max()\n",
    "    data['roll_12h_min'] = rolled_12.min()\n",
    "    \n",
    "    # 24-hour and 168-hour minimums (baseline floors)\n",
    "    data['roll_24h_min'] = rolled_24.min()\n",
    "    data['roll_168h_min'] = rolled_168.min()\n",
    "    \n",
    "    return data, rolled_6, rolled_24, rolled_168\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a421a2",
   "metadata": {},
   "source": [
    "## 7. Rolling Percentiles (4 features)\n",
    "###### Quantiles capture distribution shape and outlier context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5116f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_percentiles(data, rolled_24, rolled_168):\n",
    "    \"\"\"Calculates percentiles from rolling distributions\"\"\"\n",
    "    \n",
    "    # Weekly percentiles (long-term distribution)\n",
    "    data['roll_168h_median'] = rolled_168.median()\n",
    "    data['roll_168h_q25'] = rolled_168.quantile(0.25)\n",
    "    data['roll_168h_q75'] = rolled_168.quantile(0.75)\n",
    "    \n",
    "    # Daily median (short-term center)\n",
    "    data['roll_24h_median'] = rolled_24.median()\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbe7dc5",
   "metadata": {},
   "source": [
    "## 8. Momentum Features (3 features)\n",
    "###### Rate of change indicates acceleration or deceleration trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91472041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_momentum_features(data):\n",
    "    \"\"\"Computes change rates over different time scales\"\"\"\n",
    "    \n",
    "    # 1-hour momentum (immediate change)\n",
    "    data['momentum_1h'] = data['Appliances'] - data['lag_1h']\n",
    "    \n",
    "    # 6-hour momentum (short-term change)\n",
    "    data['momentum_6h'] = data['Appliances'] - data['lag_6h']\n",
    "    \n",
    "    # 24-hour momentum (daily change)\n",
    "    data['momentum_24h'] = data['Appliances'] - data['lag_24h']\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713eaff1",
   "metadata": {},
   "source": [
    "## 9. Relative Position Features (6 features)\n",
    "###### Normalized metrics show current value context within distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a0e801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_relative_position(data, rolled_6, rolled_168):\n",
    "    \"\"\"Creates relative positioning and z-score features\"\"\"\n",
    "    \n",
    "    # Distance from minimums (how far above baseline)\n",
    "    data['dist_from_24h_min'] = data['Appliances'] - data['roll_24h_min']\n",
    "    data['dist_from_6h_min'] = data['Appliances'] - data['roll_6h_min']\n",
    "    \n",
    "    # Ratio to means (relative magnitude)\n",
    "    data['rel_to_24h_mean'] = data['Appliances'] / (data['roll_24h_mean'] + 1)\n",
    "    data['rel_to_6h_mean'] = data['Appliances'] / (data['roll_6h_mean'] + 1)\n",
    "    \n",
    "    # Z-scores (standardized deviations)\n",
    "    data['zscore_168h'] = (data['Appliances'] - data['roll_168h_mean']) / (rolled_168.std() + 1)\n",
    "    data['zscore_6h'] = (data['Appliances'] - data['roll_6h_mean']) / (rolled_6.std() + 1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f47605",
   "metadata": {},
   "source": [
    "## 10. Volatility Feature (1 feature)\n",
    "###### Range captures variability over time window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "979acc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_volatility(data, rolled_24):\n",
    "    \"\"\"Calculates range as volatility measure\"\"\"\n",
    "    \n",
    "    # 24-hour range (max - min = daily volatility)\n",
    "    data['range_24h'] = rolled_24.max() - data['roll_24h_min']\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c592c688",
   "metadata": {},
   "source": [
    "## 11. Exponential Moving Averages (2 features)\n",
    "###### EMAs weight recent values more heavily than simple moving averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41fd44b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ema_features(data):\n",
    "    \"\"\"Creates exponentially weighted moving averages\"\"\"\n",
    "    \n",
    "    # 3-hour EMA (fast-reacting trend)\n",
    "    data['ema_3h'] = data['Appliances'].ewm(span=3, adjust=False).mean().shift(1)\n",
    "    \n",
    "    # 6-hour EMA (balanced trend)\n",
    "    data['ema_6h'] = data['Appliances'].ewm(span=6, adjust=False).mean().shift(1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7506c4",
   "metadata": {},
   "source": [
    "## 12. Usage Regime Feature (1 feature)\n",
    "###### Categorical bins identify low/medium/high/very-high usage states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aadf2ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_usage_regime(data):\n",
    "    \"\"\"Bins appliance usage into discrete regimes\"\"\"\n",
    "    \n",
    "    # Categorize into 4 usage levels (2nd most important feature!)\n",
    "    data['usage_regime'] = pd.cut(data['Appliances'], \n",
    "                                   bins=[0, 100, 200, 300, np.inf],\n",
    "                                   labels=[0, 1, 2, 3]).astype(int)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a150a50",
   "metadata": {},
   "source": [
    "## 13. Context Flag Features (5 features)\n",
    "###### Binary indicators for specific time periods and conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e27e64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_context_flags(data):\n",
    "    \"\"\"Creates binary flags for time-of-day and weekend periods\"\"\"\n",
    "    \n",
    "    # Time of day flags\n",
    "    data['is_night'] = ((data['hour'] >= 22) | (data['hour'] <= 5)).astype(int)\n",
    "    data['is_morning'] = ((data['hour'] >= 6) & (data['hour'] <= 9)).astype(int)\n",
    "    data['is_evening'] = ((data['hour'] >= 17) & (data['hour'] <= 21)).astype(int)\n",
    "    \n",
    "    # Weekend flag\n",
    "    data['is_weekend'] = (data['day_of_week'] >= 5).astype(int)\n",
    "    \n",
    "    # Target hour peak flag (high usage evening hours)\n",
    "    data['target_is_peak'] = ((data['target_hour'] >= 18) & (data['target_hour'] <= 20)).astype(int)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c38f99",
   "metadata": {},
   "source": [
    "## 14. Spike Detection Features (3 features)\n",
    "###### Identify local peaks, troughs, and anomalous intensity levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02e1a4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spike_detection(data):\n",
    "    \"\"\"Detects local extrema and spike intensity\"\"\"\n",
    "    \n",
    "    # Local peak (higher than neighbors)\n",
    "    data['is_local_peak'] = ((data['Appliances'] > data['lag_1h']) & \n",
    "                              (data['Appliances'] > data['Appliances'].shift(-1))).astype(int)\n",
    "    \n",
    "    # Local trough (lower than neighbors)\n",
    "    data['is_local_trough'] = ((data['Appliances'] < data['lag_1h']) & \n",
    "                                (data['Appliances'] < data['Appliances'].shift(-1))).astype(int)\n",
    "    \n",
    "    # Spike intensity (ratio to 24h baseline)\n",
    "    data['spike_intensity_24h'] = data['Appliances'] / (data['roll_24h_mean'] + 1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2676667",
   "metadata": {},
   "source": [
    "## 15. Interaction Features (2 features)\n",
    "###### Multiplicative features capture combined effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cb6ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_interactions(data):\n",
    "    \"\"\"Creates interaction terms between categorical and continuous features\"\"\"\n",
    "    \n",
    "    # Evening period weighted by usage level\n",
    "    data['evening_x_level'] = data['is_evening'] * data['Appliances']\n",
    "    \n",
    "    # Weekend weighted by usage level\n",
    "    data['weekend_x_level'] = data['is_weekend'] * data['Appliances']\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb34b66",
   "metadata": {},
   "source": [
    "## 16. Historical Pattern Feature (1 feature)\n",
    "###### Average usage at the same hour across previous weeks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c019aa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_historical_patterns(data):\n",
    "    \"\"\"Computes historical average for target hour\"\"\"\n",
    "    \n",
    "    # Rolling average of this hour across past weeks\n",
    "    data['avg_this_hour'] = data.groupby('target_hour')['Appliances'].transform(\n",
    "        lambda x: x.shift(1).rolling(168, min_periods=24).mean()\n",
    "    )\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278fb75c",
   "metadata": {},
   "source": [
    "## 17. Trend Feature (1 feature)\n",
    "###### Linear slope captures directional movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "293748a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trend(data):\n",
    "    \"\"\"Calculates 6-hour linear trend slope\"\"\"\n",
    "    \n",
    "    # Trend direction over recent 6 hours\n",
    "    data['trend_6h'] = data['Appliances'].shift(1).rolling(6).apply(\n",
    "        lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) == 6 else 0, \n",
    "        raw=True\n",
    "    )\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b47fae7",
   "metadata": {},
   "source": [
    "## 18. Complete Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "758855fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_all_features(df, horizon_hours=2):\n",
    "    \"\"\"\n",
    "    Master function that orchestrates all feature engineering steps\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'date' and 'Appliances' columns\n",
    "        horizon_hours: Prediction horizon (default: 2)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with 49 engineered features + target\n",
    "    \"\"\"\n",
    "    # Initialize base features\n",
    "    data = create_essential_features(df, horizon_hours)\n",
    "    \n",
    "    # Add feature groups sequentially\n",
    "    data = add_time_features(data)\n",
    "    data = add_lag_features(data)\n",
    "    data = add_rolling_means(data)\n",
    "    data, rolled_6, rolled_24, rolled_168 = add_rolling_extremes(data)\n",
    "    data = add_rolling_percentiles(data, rolled_24, rolled_168)\n",
    "    data = add_momentum_features(data)\n",
    "    data = add_relative_position(data, rolled_6, rolled_168)\n",
    "    data = add_volatility(data, rolled_24)\n",
    "    data = add_ema_features(data)\n",
    "    data = add_usage_regime(data)\n",
    "    data = add_context_flags(data)\n",
    "    data = add_spike_detection(data)\n",
    "    data = add_interactions(data)\n",
    "    data = add_historical_patterns(data)\n",
    "    data = add_trend(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef22aa4a",
   "metadata": {},
   "source": [
    "## 19. Feature List Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62465e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_names():\n",
    "    \"\"\"Returns ordered list of all 49 feature names for model training\"\"\"\n",
    "    \n",
    "    features = [\n",
    "        # Time features (8)\n",
    "        'hour_sin', 'hour_cos', 'target_hour_sin', 'dow_sin', 'month_sin',\n",
    "        'day_of_week', 'target_dow', 'month',\n",
    "        \n",
    "        # Lag features (7)\n",
    "        'lag_1h', 'lag_2h', 'lag_3h', 'lag_6h', 'lag_12h', 'lag_24h', 'lag_168h',\n",
    "        \n",
    "        # Rolling means (5)\n",
    "        'roll_3h_mean', 'roll_6h_mean', 'roll_12h_mean', 'roll_24h_mean', 'roll_168h_mean',\n",
    "        \n",
    "        # Rolling extremes (6)\n",
    "        'roll_6h_max', 'roll_6h_min', 'roll_12h_max', 'roll_12h_min', \n",
    "        'roll_24h_min', 'roll_168h_min',\n",
    "        \n",
    "        # Percentiles (4)\n",
    "        'roll_168h_median', 'roll_168h_q25', 'roll_168h_q75', 'roll_24h_median',\n",
    "        \n",
    "        # Momentum (3)\n",
    "        'momentum_1h', 'momentum_6h', 'momentum_24h',\n",
    "        \n",
    "        # Relative position (6)\n",
    "        'dist_from_24h_min', 'dist_from_6h_min', 'rel_to_24h_mean', \n",
    "        'rel_to_6h_mean', 'zscore_168h', 'zscore_6h',\n",
    "        \n",
    "        # Volatility (1)\n",
    "        'range_24h',\n",
    "        \n",
    "        # EMAs (2)\n",
    "        'ema_3h', 'ema_6h',\n",
    "        \n",
    "        # Regime (1)\n",
    "        'usage_regime',\n",
    "        \n",
    "        # Context flags (5)\n",
    "        'is_night', 'is_morning', 'is_evening', 'is_weekend', 'target_is_peak',\n",
    "        \n",
    "        # Spike detection (3)\n",
    "        'is_local_peak', 'is_local_trough', 'spike_intensity_24h',\n",
    "        \n",
    "        # Interactions (2)\n",
    "        'evening_x_level', 'weekend_x_level',\n",
    "        \n",
    "        # Historical patterns (1)\n",
    "        'avg_this_hour',\n",
    "        \n",
    "        # Trend (1)\n",
    "        'trend_6h'\n",
    "    ]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9382b3a",
   "metadata": {},
   "source": [
    "## 20. Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f414165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data shape: (19735, 29)\n",
      "Columns: ['date', 'Appliances', 'lights', 'T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4', 'RH_4', 'T5', 'RH_5', 'T6', 'RH_6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9', 'RH_9', 'T_out', 'Press_mm_hg', 'RH_out', 'Windspeed', 'Visibility', 'Tdewpoint', 'rv1', 'rv2']\n",
      "\n",
      "Applying feature engineering...\n",
      "Engineered data shape: (19735, 61)\n",
      "\n",
      "Total features created: 55\n",
      "Final dataset shape: (19735, 57)\n",
      "Missing values: 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('C:/Users/lekshmi/Desktop/ml projects/appliances energy prediction/KAG_energydata_complete.csv')\n",
    "\n",
    "print(f\"Raw data shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Apply feature engineering pipeline\n",
    "print(\"\\nApplying feature engineering...\")\n",
    "df_features = engineer_all_features(df[['date', 'Appliances']], horizon_hours=2)\n",
    "print(f\"Engineered data shape: {df_features.shape}\")\n",
    "\n",
    "features = get_feature_names()\n",
    "print(f\"\\nTotal features created: {len(features)}\")\n",
    "\n",
    "# Prepare final dataset\n",
    "X = df_features[features].fillna(0)\n",
    "y = df_features['target']\n",
    "\n",
    "# Combine features and target for saving\n",
    "final_data = X.copy()\n",
    "final_data['target'] = y\n",
    "final_data['date'] = df_features['date']\n",
    "\n",
    "# Reorder columns (date first, target last)\n",
    "cols = ['date'] + features + ['target']\n",
    "final_data = final_data[cols]\n",
    "\n",
    "print(f\"Final dataset shape: {final_data.shape}\")\n",
    "final_data=final_data.dropna()\n",
    "print(f\"Missing values: {final_data.isnull().sum().sum()}\")\n",
    "final_data=final_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84795a52",
   "metadata": {},
   "source": [
    "## 21. Save Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6394aef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved processed features to: data/processed/engineered_features.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "\n",
    "# Save complete engineered dataset\n",
    "output_path = 'data/processed/engineered_features.csv'\n",
    "final_data.to_csv(output_path, index=False)\n",
    "print(f\"✓ Saved processed features to: {output_path}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "baba93fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved feature names to: data/features/feature_names.txt\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('data/features', exist_ok=True)\n",
    "feature_names_path = 'data/features/feature_names.txt'\n",
    "with open(feature_names_path, 'w') as f:\n",
    "    f.write('\\n'.join(features))\n",
    "print(f\"✓ Saved feature names to: {feature_names_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "energy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
